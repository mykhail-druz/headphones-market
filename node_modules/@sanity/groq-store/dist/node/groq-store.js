var EventSourcePolyfill = require('eventsource');
var groq = require('groq');
var deepEqual = require('fast-deep-equal');
var throttleDebounce = require('throttle-debounce');
var groqJs = require('groq-js');
var mendoza = require('mendoza');
var split = require('split2');
var get = require('simple-get');

function _interopDefaultLegacy (e) { return e && typeof e === 'object' && 'default' in e ? e : { 'default': e }; }

var EventSourcePolyfill__default = /*#__PURE__*/_interopDefaultLegacy(EventSourcePolyfill);
var groq__default = /*#__PURE__*/_interopDefaultLegacy(groq);
var deepEqual__default = /*#__PURE__*/_interopDefaultLegacy(deepEqual);
var split__default = /*#__PURE__*/_interopDefaultLegacy(split);
var get__default = /*#__PURE__*/_interopDefaultLegacy(get);

const isNativeBrowserEventSource = eventSource => typeof window !== 'undefined' && eventSource.addEventListener === window.EventSource.prototype.addEventListener;
const addEventSourceListener = (eventSource, type, listener) => {
  if (isNativeBrowserEventSource(eventSource)) {
    eventSource.addEventListener(type, listener, false);
  }
  // Polyfilled event source does not accept option parameter
  eventSource.addEventListener(type, listener);
};
function listen(EventSourceImpl, config, handlers) {
  const {
    projectId,
    dataset,
    token
  } = config;
  const headers = token ? {
    Authorization: `Bearer ${token}`
  } : undefined;
  const url = `https://${projectId}.api.sanity.io/v1/data/listen/${dataset}?query=*&effectFormat=mendoza`;
  const es = new EventSourceImpl(url, {
    withCredentials: true,
    headers
  });
  addEventSourceListener(es, 'welcome', handlers.open);
  addEventSourceListener(es, 'mutation', getMutationParser(handlers.next));
  addEventSourceListener(es, 'channelError', msg => {
    es.close();
    let data;
    try {
      data = JSON.parse(msg.data);
    } catch (err) {
      handlers.error(new Error('Unknown error parsing listener message'));
      return;
    }
    handlers.error(new Error(data.message || data.error || `Listener returned HTTP ${data.statusCode}`));
  });
  addEventSourceListener(es, 'error', err => {
    const origin = typeof window !== 'undefined' && window.location.origin;
    const hintSuffix = origin ? `, and that the CORS-origin (${origin}) is allowed` : '';
    const errorMessage = isErrorLike(err) ? ` (${err.message})` : '';
    handlers.error(new Error(`Error establishing listener - check that the project ID and dataset are correct${hintSuffix}${errorMessage}`));
  });
  return {
    unsubscribe: () => Promise.resolve(es.close())
  };
}
function getMutationParser(cb) {
  return msg => {
    let data;
    try {
      data = JSON.parse(msg.data);
    } catch (err) {
      // intentional noop
      return;
    }
    cb(data);
  };
}
function isErrorLike(err) {
  return typeof err === 'object' && err !== null && 'message' in err;
}

function isDraft(doc) {
  return doc._id.startsWith('drafts.');
}
function getPublishedId(document) {
  return isDraft(document) ? document._id.slice(7) : document._id;
}

function applyPatchWithoutRev(doc, patch) {
  const patchDoc = {
    ...doc
  };
  delete patchDoc._rev;
  return mendoza.applyPatch(patchDoc, patch);
}

const DEBOUNCE_MS = 25;
function noop() {
  return Promise.resolve();
}
function getSyncingDataset(config, onNotifyUpdate, {
  getDocuments,
  EventSource
}) {
  const {
    projectId,
    dataset,
    listen: useListener,
    overlayDrafts,
    documentLimit,
    token,
    includeTypes
  } = config;
  if (!useListener) {
    const loaded = getDocuments({
      projectId,
      dataset,
      documentLimit,
      token,
      includeTypes
    }).then(onUpdate).then(noop);
    return {
      unsubscribe: noop,
      loaded
    };
  }
  const indexedDocuments = new Map();
  // undefined until the listener has been set up and the initial export is done
  let documents;
  // holds any mutations that happen while fetching documents so they can be applied after updates
  const buffer = [];
  // Return a promise we can resolve once we've established a listener and reconciled any mutations
  let onDoneLoading;
  let onLoadError;
  const loaded = new Promise((resolve, reject) => {
    onDoneLoading = resolve;
    onLoadError = reject;
  });
  // We don't want to flush updates while we're in the same transaction, so a normal
  // throttle/debounce wouldn't do it. We need to wait and see if the next mutation is
  // within the same transaction as the previous, and if not we can flush. Of course,
  // we can't wait forever, so an upper threshold of X ms should be counted as "ok to flush"
  let stagedDocs;
  let previousTrx;
  let flushTimeout;
  const listener = listen(EventSource, config, {
    next: onMutationReceived,
    open: onOpen,
    error: error => onLoadError(error)
  });
  return {
    unsubscribe: listener.unsubscribe,
    loaded
  };
  async function onOpen() {
    const initial = await getDocuments({
      projectId,
      dataset,
      documentLimit,
      token,
      includeTypes
    });
    documents = applyBufferedMutations(initial, buffer);
    documents.forEach(doc => indexedDocuments.set(doc._id, doc));
    onUpdate(documents);
    onDoneLoading();
  }
  function onMutationReceived(msg) {
    if (documents) {
      applyMutation(msg);
      scheduleUpdate(documents, msg);
    } else {
      buffer.push(msg);
    }
  }
  function scheduleUpdate(docs, msg) {
    clearTimeout(flushTimeout);
    if (previousTrx !== msg.transactionId && stagedDocs) {
      // This is a new transaction, meaning we can immediately flush any pending
      // doc updates if there are any
      onUpdate(stagedDocs);
      previousTrx = undefined;
    } else {
      previousTrx = msg.transactionId;
      stagedDocs = docs.slice();
    }
    flushTimeout = setTimeout(onUpdate, DEBOUNCE_MS, docs.slice());
  }
  function onUpdate(docs) {
    stagedDocs = undefined;
    flushTimeout = undefined;
    previousTrx = undefined;
    onNotifyUpdate(overlayDrafts ? overlay(docs) : docs);
  }
  function applyMutation(msg) {
    if (!msg.effects || msg.documentId.startsWith('_.')) {
      return;
    }
    const document = indexedDocuments.get(msg.documentId) || null;
    replaceDocument(msg.documentId, applyPatchWithoutRev(document, msg.effects.apply));
  }
  function replaceDocument(id, document) {
    const current = indexedDocuments.get(id);
    const docs = documents || [];
    const position = current ? docs.indexOf(current) : -1;
    if (position === -1 && document) {
      // Didn't exist previously, but was now created. Add it.
      docs.push(document);
      indexedDocuments.set(id, document);
    } else if (document) {
      // Existed previously and still does. Replace it.
      docs.splice(position, 1, document);
      indexedDocuments.set(id, document);
    } else {
      // Existed previously, but is now deleted. Remove it.
      docs.splice(position, 1);
      indexedDocuments.delete(id);
    }
  }
}
function applyBufferedMutations(documents, mutations) {
  // Group by document ID
  const groups = new Map();
  mutations.forEach(mutation => {
    const group = groups.get(mutation.documentId) || [];
    group.push(mutation);
    groups.set(mutation.documentId, group);
  });
  // Discard all mutations that happened before our current document
  groups.forEach((group, id) => {
    const document = documents.find(doc => doc._id === id);
    if (!document) {
      // @todo handle
      // eslint-disable-next-line no-console
      console.warn('Received mutation for missing document %s', id);
      return;
    }
    // Mutations are sorted by timestamp, apply any that arrived after
    // we fetched the initial documents
    let hasFoundRevision = false;
    let current = document;
    group.forEach(mutation => {
      hasFoundRevision = hasFoundRevision || mutation.previousRev === document._rev;
      if (!hasFoundRevision) {
        return;
      }
      if (mutation.effects) {
        current = applyPatchWithoutRev(current, mutation.effects.apply);
      }
    });
    // Replace the indexed documents
    documents.splice(documents.indexOf(document), 1, current);
  });
  return documents;
}
function overlay(documents) {
  const overlayed = new Map();
  documents.forEach(doc => {
    const existing = overlayed.get(getPublishedId(doc));
    if (doc._id.startsWith('drafts.')) {
      // Drafts always overlay
      overlayed.set(getPublishedId(doc), pretendThatItsPublished(doc));
    } else if (!existing) {
      // Published documents only override if draft doesn't exist
      overlayed.set(doc._id, doc);
    }
  });
  return Array.from(overlayed.values());
}
// Strictly speaking it would be better to allow groq-js to resolve `draft.<id>`,
// but for now this will have to do
function pretendThatItsPublished(doc) {
  return {
    ...doc,
    _id: getPublishedId(doc)
  };
}

function groqStore$1(config, envImplementations) {
  let documents = [];
  const executeThrottled = throttleDebounce.throttle(config.subscriptionThrottleMs || 50, executeAllSubscriptions);
  const activeSubscriptions = [];
  let dataset;
  async function loadDataset() {
    if (!dataset) {
      dataset = getSyncingDataset(config, docs => {
        documents = docs;
        executeThrottled();
      }, envImplementations);
    }
    await dataset.loaded;
  }
  async function query(groqQuery, params) {
    await loadDataset();
    const tree = groqJs.parse(groqQuery, {
      params
    });
    const result = await groqJs.evaluate(tree, {
      dataset: documents,
      params
    });
    return result.get();
  }
  async function getDocument(documentId) {
    await loadDataset();
    return query(groq__default["default"]`*[_id == $id][0]`, {
      id: documentId
    });
  }
  async function getDocuments(documentIds) {
    await loadDataset();
    const subQueries = documentIds.map(id => `*[_id == "${id}"][0]`).join(',\n');
    return query(`[${subQueries}]`);
  }
  function subscribe(groqQuery, params, callback) {
    if (!config.listen) {
      throw new Error('Cannot use `subscribe()` without `listen: true`');
    }
    // @todo Execute the query against an empty dataset for validation purposes
    // Store the subscription so we can re-run the query on new data
    const subscription = {
      query: groqQuery,
      params,
      callback
    };
    activeSubscriptions.push(subscription);
    let unsubscribed = false;
    const unsubscribe = () => {
      if (unsubscribed) {
        return Promise.resolve();
      }
      unsubscribed = true;
      activeSubscriptions.splice(activeSubscriptions.indexOf(subscription), 1);
      return Promise.resolve();
    };
    executeQuerySubscription(subscription);
    return {
      unsubscribe
    };
  }
  function executeQuerySubscription(subscription) {
    return query(subscription.query, subscription.params).then(res => {
      if ('previousResult' in subscription && deepEqual__default["default"](subscription.previousResult, res)) {
        return;
      }
      subscription.previousResult = res;
      subscription.callback(undefined, res);
    }).catch(err => {
      subscription.callback(err);
    });
  }
  function executeAllSubscriptions() {
    activeSubscriptions.forEach(executeQuerySubscription);
  }
  function close() {
    executeThrottled.cancel();
    return dataset ? dataset.unsubscribe() : Promise.resolve();
  }
  return {
    query,
    getDocument,
    getDocuments,
    subscribe,
    close
  };
}

function isStreamError(result) {
  if (!result) {
    return false;
  }
  if (!('error' in result) || typeof result.error !== 'object' || result.error === null) {
    return false;
  }
  return 'description' in result.error && typeof result.error.description === 'string' && !('_id' in result);
}
function getError(body) {
  if (typeof body === 'object' && 'error' in body && 'message' in body) {
    return body.message || body.error;
  }
  return '<unknown error>';
}
function isRelevantDocument(doc) {
  return !doc._id.startsWith('_.');
}

const getDocuments = function getDocuments({
  projectId,
  dataset,
  token,
  documentLimit,
  includeTypes = []
}) {
  const baseUrl = `https://${projectId}.api.sanity.io/v1/data/export/${dataset}`;
  const params = includeTypes.length > 0 ? new URLSearchParams({
    types: includeTypes == null ? void 0 : includeTypes.join(',')
  }) : '';
  const url = `${baseUrl}?${params}`;
  const headers = token ? {
    Authorization: `Bearer ${token}`
  } : undefined;
  return new Promise((resolve, reject) => {
    get__default["default"]({
      url,
      headers
    }, (err, response) => {
      if (err) {
        reject(err);
        return;
      }
      response.setEncoding('utf8');
      const chunks = [];
      if (response.statusCode !== 200) {
        response.on('data', chunk => chunks.push(chunk)).on('end', () => {
          const body = JSON.parse(Buffer.concat(chunks).toString('utf8'));
          reject(new Error(`Error streaming dataset: ${getError(body)}`));
        });
        return;
      }
      const documents = [];
      response.pipe(split__default["default"](JSON.parse)).on('data', doc => {
        if (isStreamError(doc)) {
          reject(new Error(`Error streaming dataset: ${doc.error}`));
          return;
        }
        if (doc && isRelevantDocument(doc)) {
          documents.push(doc);
        }
        if (documentLimit && documents.length > documentLimit) {
          reject(new Error(`Error streaming dataset: Reached limit of ${documentLimit} documents`));
          response.destroy();
        }
      }).on('end', () => resolve(documents));
    });
  });
};

function assertEnvSupport() {
  const [major] = process.version.replace(/^v/, '').split('.', 1).map(Number);
  if (major < 10) {
    throw new Error('Node.js version 10 or higher required');
  }
}

/**
 * Note: Entry point for _browser_ build is in browser/index.ts
 */
function groqStore(config) {
  var _config$EventSource;
  assertEnvSupport();
  return groqStore$1(config, {
    EventSource: (_config$EventSource = config.EventSource) != null ? _config$EventSource : EventSourcePolyfill__default["default"],
    getDocuments
  });
}

Object.defineProperty(exports, 'groq', {
  enumerable: true,
  get: function () { return groq__default["default"]; }
});
exports.groqStore = groqStore;
//# sourceMappingURL=groq-store.js.map
