{"version":3,"file":"groq-store.js","sources":["../../src/listen.ts","../../src/drafts.ts","../../src/patch.ts","../../src/syncingDataset.ts","../../src/browser/getDocuments.ts","../../src/browser/index.ts","../../src/browser/support.ts","../../src/groqStore.ts"],"sourcesContent":["import {Subscription, MutationEvent, Config, ApiError, EnvImplementations} from './types'\n\ntype EventSourceInstance = InstanceType<EnvImplementations['EventSource']>\n\nconst isNativeBrowserEventSource = (\n  eventSource: EventSourceInstance\n): eventSource is InstanceType<typeof globalThis.EventSource> =>\n  typeof window !== 'undefined' &&\n  eventSource.addEventListener === window.EventSource.prototype.addEventListener\n\nconst addEventSourceListener = (\n  eventSource: EventSourceInstance,\n  type: string,\n  listener: EventListener\n): void => {\n  if (isNativeBrowserEventSource(eventSource)) {\n    eventSource.addEventListener(type, listener, false)\n  }\n\n  // Polyfilled event source does not accept option parameter\n  eventSource.addEventListener(type, listener)\n}\n\nexport function listen(\n  EventSourceImpl: EnvImplementations['EventSource'],\n  config: Config,\n  handlers: {\n    open: () => void\n    error: (err: Error) => void\n    next: (event: MutationEvent) => void\n  }\n): Subscription {\n  const {projectId, dataset, token} = config\n  const headers = token ? {Authorization: `Bearer ${token}`} : undefined\n  const url = `https://${projectId}.api.sanity.io/v1/data/listen/${dataset}?query=*&effectFormat=mendoza`\n  const es = new EventSourceImpl(url, {withCredentials: true, headers})\n\n  addEventSourceListener(es, 'welcome', handlers.open)\n\n  addEventSourceListener(es, 'mutation', getMutationParser(handlers.next))\n\n  addEventSourceListener(es, 'channelError', (msg: any) => {\n    es.close()\n\n    let data\n    try {\n      data = JSON.parse(msg.data) as ApiError\n    } catch (err) {\n      handlers.error(new Error('Unknown error parsing listener message'))\n      return\n    }\n\n    handlers.error(\n      new Error(data.message || data.error || `Listener returned HTTP ${data.statusCode}`)\n    )\n  })\n\n  addEventSourceListener(es, 'error', (err: Event) => {\n    const origin = typeof window !== 'undefined' && window.location.origin\n    const hintSuffix = origin ? `, and that the CORS-origin (${origin}) is allowed` : ''\n    const errorMessage = isErrorLike(err) ? ` (${err.message})` : ''\n    handlers.error(\n      new Error(\n        `Error establishing listener - check that the project ID and dataset are correct${hintSuffix}${errorMessage}`\n      )\n    )\n  })\n\n  return {\n    unsubscribe: (): Promise<void> => Promise.resolve(es.close()),\n  }\n}\n\nfunction getMutationParser(cb: (event: MutationEvent) => void): (msg: any) => void {\n  return (msg: any) => {\n    let data\n    try {\n      data = JSON.parse(msg.data)\n    } catch (err) {\n      // intentional noop\n      return\n    }\n\n    cb(data)\n  }\n}\n\nfunction isErrorLike(err: unknown): err is {message: string} {\n  return typeof err === 'object' && err !== null && 'message' in err\n}\n","import {SanityDocument} from '@sanity/types'\n\nexport function isDraft(doc: SanityDocument): boolean {\n  return doc._id.startsWith('drafts.')\n}\n\nexport function getPublishedId(document: SanityDocument): string {\n  return isDraft(document) ? document._id.slice(7) : document._id\n}\n","import {SanityDocument} from '@sanity/types'\nimport {applyPatch} from 'mendoza'\n\nexport function applyPatchWithoutRev(\n  doc: SanityDocument | null,\n  patch: unknown[]\n): SanityDocument | null {\n  const patchDoc = {...doc} as Omit<SanityDocument, '_rev'>\n  delete patchDoc._rev\n  return applyPatch(patchDoc, patch)\n}\n","import {SanityDocument} from '@sanity/types'\nimport {listen} from './listen'\nimport {getPublishedId} from './drafts'\nimport {applyPatchWithoutRev} from './patch'\nimport {Config, EnvImplementations, MutationEvent, Subscription} from './types'\n\nconst DEBOUNCE_MS = 25\n\nfunction noop() {\n  return Promise.resolve()\n}\n\nexport function getSyncingDataset(\n  config: Config,\n  onNotifyUpdate: (docs: SanityDocument[]) => void,\n  {getDocuments, EventSource}: EnvImplementations\n): Subscription & {loaded: Promise<void>} {\n  const {\n    projectId,\n    dataset,\n    listen: useListener,\n    overlayDrafts,\n    documentLimit,\n    token,\n    includeTypes,\n  } = config\n\n  if (!useListener) {\n    const loaded = getDocuments({projectId, dataset, documentLimit, token, includeTypes})\n      .then(onUpdate)\n      .then(noop)\n    return {unsubscribe: noop, loaded}\n  }\n\n  const indexedDocuments = new Map<string, SanityDocument>()\n\n  // undefined until the listener has been set up and the initial export is done\n  let documents: SanityDocument[] | undefined\n\n  // holds any mutations that happen while fetching documents so they can be applied after updates\n  const buffer: MutationEvent[] = []\n\n  // Return a promise we can resolve once we've established a listener and reconciled any mutations\n  let onDoneLoading: () => void\n  let onLoadError: (error: Error) => void\n  const loaded = new Promise<void>((resolve, reject) => {\n    onDoneLoading = resolve\n    onLoadError = reject\n  })\n\n  // We don't want to flush updates while we're in the same transaction, so a normal\n  // throttle/debounce wouldn't do it. We need to wait and see if the next mutation is\n  // within the same transaction as the previous, and if not we can flush. Of course,\n  // we can't wait forever, so an upper threshold of X ms should be counted as \"ok to flush\"\n  let stagedDocs: SanityDocument[] | undefined\n  let previousTrx: string | undefined\n  let flushTimeout: NodeJS.Timer | undefined\n\n  const listener = listen(EventSource, config, {\n    next: onMutationReceived,\n    open: onOpen,\n    error: (error: Error) => onLoadError(error),\n  })\n\n  return {unsubscribe: listener.unsubscribe, loaded}\n\n  async function onOpen() {\n    const initial = await getDocuments({projectId, dataset, documentLimit, token, includeTypes})\n    documents = applyBufferedMutations(initial, buffer)\n    documents.forEach((doc) => indexedDocuments.set(doc._id, doc))\n    onUpdate(documents)\n    onDoneLoading()\n  }\n\n  function onMutationReceived(msg: MutationEvent) {\n    if (documents) {\n      applyMutation(msg)\n      scheduleUpdate(documents, msg)\n    } else {\n      buffer.push(msg)\n    }\n  }\n\n  function scheduleUpdate(docs: SanityDocument[], msg: MutationEvent) {\n    clearTimeout(flushTimeout)\n\n    if (previousTrx !== msg.transactionId && stagedDocs) {\n      // This is a new transaction, meaning we can immediately flush any pending\n      // doc updates if there are any\n      onUpdate(stagedDocs)\n      previousTrx = undefined\n    } else {\n      previousTrx = msg.transactionId\n      stagedDocs = docs.slice()\n    }\n\n    flushTimeout = setTimeout(onUpdate, DEBOUNCE_MS, docs.slice())\n  }\n\n  function onUpdate(docs: SanityDocument[]) {\n    stagedDocs = undefined\n    flushTimeout = undefined\n    previousTrx = undefined\n    onNotifyUpdate(overlayDrafts ? overlay(docs) : docs)\n  }\n\n  function applyMutation(msg: MutationEvent) {\n    if (!msg.effects || msg.documentId.startsWith('_.')) {\n      return\n    }\n\n    const document = indexedDocuments.get(msg.documentId) || null\n    replaceDocument(msg.documentId, applyPatchWithoutRev(document, msg.effects.apply))\n  }\n\n  function replaceDocument(id: string, document: SanityDocument | null) {\n    const current = indexedDocuments.get(id)\n    const docs = documents || []\n    const position = current ? docs.indexOf(current) : -1\n\n    if (position === -1 && document) {\n      // Didn't exist previously, but was now created. Add it.\n      docs.push(document)\n      indexedDocuments.set(id, document)\n    } else if (document) {\n      // Existed previously and still does. Replace it.\n      docs.splice(position, 1, document)\n      indexedDocuments.set(id, document)\n    } else {\n      // Existed previously, but is now deleted. Remove it.\n      docs.splice(position, 1)\n      indexedDocuments.delete(id)\n    }\n  }\n}\n\nfunction applyBufferedMutations(\n  documents: SanityDocument[],\n  mutations: MutationEvent[]\n): SanityDocument[] {\n  // Group by document ID\n  const groups = new Map<string, MutationEvent[]>()\n  mutations.forEach((mutation) => {\n    const group = groups.get(mutation.documentId) || []\n    group.push(mutation)\n    groups.set(mutation.documentId, group)\n  })\n\n  // Discard all mutations that happened before our current document\n  groups.forEach((group, id) => {\n    const document = documents.find((doc) => doc._id === id)\n    if (!document) {\n      // @todo handle\n      // eslint-disable-next-line no-console\n      console.warn('Received mutation for missing document %s', id)\n      return\n    }\n\n    // Mutations are sorted by timestamp, apply any that arrived after\n    // we fetched the initial documents\n    let hasFoundRevision = false\n    let current: SanityDocument | null = document\n    group.forEach((mutation) => {\n      hasFoundRevision = hasFoundRevision || mutation.previousRev === document._rev\n      if (!hasFoundRevision) {\n        return\n      }\n\n      if (mutation.effects) {\n        current = applyPatchWithoutRev(current, mutation.effects.apply)\n      }\n    })\n\n    // Replace the indexed documents\n    documents.splice(documents.indexOf(document), 1, current)\n  })\n\n  return documents\n}\n\nfunction overlay(documents: SanityDocument[]): SanityDocument[] {\n  const overlayed = new Map<string, SanityDocument>()\n\n  documents.forEach((doc) => {\n    const existing = overlayed.get(getPublishedId(doc))\n    if (doc._id.startsWith('drafts.')) {\n      // Drafts always overlay\n      overlayed.set(getPublishedId(doc), pretendThatItsPublished(doc))\n    } else if (!existing) {\n      // Published documents only override if draft doesn't exist\n      overlayed.set(doc._id, doc)\n    }\n  })\n\n  return Array.from(overlayed.values())\n}\n\n// Strictly speaking it would be better to allow groq-js to resolve `draft.<id>`,\n// but for now this will have to do\nfunction pretendThatItsPublished(doc: SanityDocument): SanityDocument {\n  return {...doc, _id: getPublishedId(doc)}\n}\n","import {SanityDocument} from '@sanity/types'\nimport type {ReadableStreamDefaultReadResult} from 'stream/web'\nimport {EnvImplementations} from '../types'\n\ntype StreamError = {error: {description?: string; type: string}}\ntype StreamResult = SanityDocument | StreamError\n\nexport const getDocuments: EnvImplementations['getDocuments'] = async function getDocuments({\n  projectId,\n  dataset,\n  token,\n  documentLimit,\n  includeTypes = [],\n}: {\n  projectId: string\n  dataset: string\n  token?: string\n  documentLimit?: number\n  includeTypes?: string[]\n}): Promise<SanityDocument[]> {\n  const baseUrl = `https://${projectId}.api.sanity.io/v1/data/export/${dataset}`\n  const params =\n    includeTypes.length > 0 ? new URLSearchParams({types: includeTypes?.join(',')}) : ''\n  const url = `${baseUrl}?${params}`\n  const headers = token ? {Authorization: `Bearer ${token}`} : undefined\n  const response = await fetch(url, {credentials: 'include', headers})\n\n  if (response.status !== 200) {\n    throw new Error(`Error streaming dataset: ${getError(await response.json())}`)\n  }\n\n  const stream = getDocumentStream(response.body)\n  const reader = stream.getReader()\n\n  const documents: SanityDocument[] = []\n  let result\n  let document\n  do {\n    result = await reader.read()\n    document = result.value\n\n    if (isStreamError(document)) {\n      throw new Error(`Error streaming dataset: ${document.error}`)\n    } else if (document && isRelevantDocument(document)) {\n      documents.push(document)\n    }\n\n    if (documentLimit && documents.length > documentLimit) {\n      reader.cancel('Reached document limit')\n      throw new Error(\n        `Error streaming dataset: Reached limit of ${documentLimit} documents. Try using the includeTypes option to reduce the amount of documents, or increase the limit.`\n      )\n    }\n  } while (!result.done)\n\n  return documents\n}\n\nfunction getDocumentStream(body: Response['body']): ReadableStream<StreamResult> {\n  if (!body) {\n    throw new Error('Failed to read body from response')\n  }\n\n  let reader: ReadableStreamDefaultReader<Uint8Array> | undefined\n  let cancelled = false\n\n  function cancel() {\n    cancelled = true\n    if (reader) {\n      reader.cancel()\n    }\n  }\n\n  return new ReadableStream<SanityDocument>({\n    start(controller): void | PromiseLike<void> {\n      reader = body.getReader()\n      const decoder = new TextDecoder()\n      let buffer = ''\n\n      reader\n        .read()\n        .then(processResult)\n        .catch((err) => controller.error(err))\n\n      async function processResult(\n        result: ReadableStreamDefaultReadResult<Uint8Array>\n      ): Promise<void> {\n        if (result.done) {\n          if (cancelled) {\n            return\n          }\n\n          buffer = buffer.trim()\n          if (buffer.length === 0) {\n            controller.close()\n            return\n          }\n\n          controller.enqueue(JSON.parse(buffer))\n          controller.close()\n          return\n        }\n\n        buffer += decoder.decode(result.value, {stream: true})\n        const lines = buffer.split('\\n')\n\n        for (let i = 0; i < lines.length - 1; ++i) {\n          const line = lines[i].trim()\n          if (line.length === 0) {\n            continue\n          }\n\n          try {\n            controller.enqueue(JSON.parse(line))\n          } catch (err) {\n            controller.error(err)\n            cancel()\n            return\n          }\n        }\n\n        buffer = lines[lines.length - 1]\n\n        if (!reader) {\n          return\n        }\n\n        try {\n          processResult(await reader.read())\n        } catch (err) {\n          controller.error(err)\n        }\n      }\n    },\n\n    cancel,\n  })\n}\n\nfunction isStreamError(result: StreamResult | undefined): result is StreamError {\n  if (!result) {\n    return false\n  }\n\n  if (!('error' in result) || typeof result.error !== 'object' || result.error === null) {\n    return false\n  }\n\n  return (\n    'description' in result.error &&\n    typeof (result as StreamError).error.description === 'string' &&\n    !('_id' in result)\n  )\n}\n\nfunction getError(body: any): string {\n  if (typeof body === 'object' && 'error' in body && 'message' in body) {\n    return body.message || body.error\n  }\n\n  return '<unknown error>'\n}\n\nfunction isRelevantDocument(doc: SanityDocument): boolean {\n  return !doc._id.startsWith('_.')\n}\n","import {groqStore as groqStoreApi} from '../groqStore'\nimport {Config, GroqStore} from '../types'\nimport {getDocuments} from './getDocuments'\nimport {assertEnvSupport} from './support'\n\nexport function groqStore(config: Config): GroqStore {\n  assertEnvSupport()\n\n  const EventSource = config.EventSource ?? window.EventSource\n\n  if (config.token) {\n    if (!config.EventSource) {\n      throw new Error(\n        'When the `token` option is used the `EventSource` option must also be provided.'\n      )\n    }\n    if (config.EventSource === window.EventSource)\n      throw new Error(\n        'When the `token` option is used the `EventSource` option must also be provided. ' +\n          'EventSource cannot be `window.EventSource`, as it does not support passing a token.'\n      )\n  }\n\n  return groqStoreApi(config, {\n    EventSource,\n    getDocuments,\n  })\n}\n\nexport {default as groq} from 'groq'\nexport {Subscription, GroqStore} from '../types'\n","export function assertEnvSupport(): void {\n  const required = ['EventSource', 'ReadableStream', 'fetch']\n  const unsupported = required.filter((api) => !(api in window))\n\n  if (unsupported.length > 0) {\n    throw new Error(`Browser not supported. Missing browser APIs: ${unsupported.join(', ')}`)\n  }\n}\n","import groq from 'groq'\nimport deepEqual from 'fast-deep-equal'\nimport {throttle} from 'throttle-debounce'\nimport {SanityDocument} from '@sanity/types'\nimport {parse, evaluate} from 'groq-js'\nimport {Config, EnvImplementations, GroqSubscription, GroqStore, Subscription} from './types'\nimport {getSyncingDataset} from './syncingDataset'\n\nexport function groqStore(config: Config, envImplementations: EnvImplementations): GroqStore {\n  let documents: SanityDocument[] = []\n  const executeThrottled = throttle(config.subscriptionThrottleMs || 50, executeAllSubscriptions)\n  const activeSubscriptions: GroqSubscription[] = []\n\n  let dataset: Subscription & {loaded: Promise<void>}\n\n  async function loadDataset() {\n    if (!dataset) {\n      dataset = getSyncingDataset(\n        config,\n        (docs) => {\n          documents = docs\n          executeThrottled()\n        },\n        envImplementations\n      )\n    }\n\n    await dataset.loaded\n  }\n\n  async function query<R = any>(groqQuery: string, params?: Record<string, unknown>): Promise<R> {\n    await loadDataset()\n    const tree = parse(groqQuery, {params})\n    const result = await evaluate(tree as any, {dataset: documents, params})\n    return result.get()\n  }\n\n  async function getDocument(documentId: string): Promise<SanityDocument | null> {\n    await loadDataset()\n    return query(groq`*[_id == $id][0]`, {id: documentId})\n  }\n\n  async function getDocuments(documentIds: string[]): Promise<(SanityDocument | null)[]> {\n    await loadDataset()\n    const subQueries = documentIds.map((id) => `*[_id == \"${id}\"][0]`).join(',\\n')\n    return query(`[${subQueries}]`)\n  }\n\n  function subscribe<R = any>(\n    groqQuery: string,\n    params: Record<string, unknown>,\n    callback: (error: Error | undefined, result?: R) => void\n  ): Subscription {\n    if (!config.listen) {\n      throw new Error('Cannot use `subscribe()` without `listen: true`')\n    }\n\n    // @todo Execute the query against an empty dataset for validation purposes\n\n    // Store the subscription so we can re-run the query on new data\n    const subscription = {query: groqQuery, params, callback}\n    activeSubscriptions.push(subscription)\n\n    let unsubscribed = false\n    const unsubscribe = () => {\n      if (unsubscribed) {\n        return Promise.resolve()\n      }\n\n      unsubscribed = true\n      activeSubscriptions.splice(activeSubscriptions.indexOf(subscription), 1)\n      return Promise.resolve()\n    }\n\n    executeQuerySubscription(subscription)\n    return {unsubscribe}\n  }\n\n  function executeQuerySubscription(subscription: GroqSubscription) {\n    return query(subscription.query, subscription.params)\n      .then((res) => {\n        if ('previousResult' in subscription && deepEqual(subscription.previousResult, res)) {\n          return\n        }\n\n        subscription.previousResult = res\n        subscription.callback(undefined, res)\n      })\n      .catch((err) => {\n        subscription.callback(err)\n      })\n  }\n\n  function executeAllSubscriptions() {\n    activeSubscriptions.forEach(executeQuerySubscription)\n  }\n\n  function close() {\n    executeThrottled.cancel()\n    return dataset ? dataset.unsubscribe() : Promise.resolve()\n  }\n\n  return {query, getDocument, getDocuments, subscribe, close}\n}\n"],"names":["addEventSourceListener","eventSource","type","listener","window","addEventListener","EventSource","prototype","isNativeBrowserEventSource","listen","EventSourceImpl","config","handlers","cb","token","es","projectId","dataset","withCredentials","headers","Authorization","undefined","open","next","msg","data","JSON","parse","err","close","error","Error","message","statusCode","origin","location","hintSuffix","errorMessage","isErrorLike","unsubscribe","Promise","resolve","getPublishedId","document","_id","startsWith","slice","applyPatchWithoutRev","doc","patch","patchDoc","_rev","applyPatch","noop","pact","state","value","s","o","_settle","bind","v","then","observer","_Pact","onFulfilled","onRejected","this","callback","e","_this","result","thenable","getDocuments","_ref","documentLimit","includeTypes","_ref$includeTypes","baseUrl","params","length","URLSearchParams","types","join","fetch","credentials","response","_temp3","_result","body","cancelled","reader","cancel","ReadableStream","start","controller","getReader","read","processResult","done","buffer","trim","enqueue","lines","decoder","decode","stream","split","i","line","_temp5","_reader$read2","_catch","reject","getDocumentStream","documents","_temp","_do","_reader$read","description","isStreamError","push","_temp2","status","json","_response$json","groqStore","_config$EventSource","unsupported","filter","api","assertEnvSupport","envImplementations","query","groqQuery","loadDataset","tree","evaluate","get","getSyncingDataset","onNotifyUpdate","onOpen","overlayDrafts","loaded","onUpdate","onDoneLoading","onLoadError","stagedDocs","previousTrx","flushTimeout","indexedDocuments","Map","effects","documentId","id","current","docs","position","indexOf","set","splice","replaceDocument","apply","applyMutation","clearTimeout","transactionId","setTimeout","scheduleUpdate","initial","mutations","forEach","mutation","groups","group","find","hasFoundRevision","previousRev","console","warn","applyBufferedMutations","overlayed","existing","pretendThatItsPublished","Array","from","values","overlay","executeThrottled","throttle","subscriptionThrottleMs","activeSubscriptions","executeQuerySubscription","subscription","res","deepEqual","previousResult","getDocument","groq","_templateObject","documentIds","subQueries","map","subscribe","unsubscribed"],"mappings":"odAIA,MAMMA,EAAyB,SAC7BC,EACAC,EACAC,IATiC,SACjCF,SAEkB,oBAAXG,QACPH,EAAYI,mBAAqBD,OAAOE,YAAYC,UAAUF,gBAAgB,EAO1EG,CAA2BP,IAC7BA,EAAYI,iBAAiBH,EAAMC,GAAU,GAI/CF,EAAYI,iBAAiBH,EAAMC,EACrC,WAEsBM,EACpBC,EACAC,EACAC,GAMA,IAyCyBC,EAzCEC,EAASH,EAATG,MAGrBC,EAAK,IAAIL,aAHqBC,EAA7BK,UAEyB,iCAFIL,EAAlBM,QAEsD,gCACpC,CAACC,iBAAiB,EAAMC,QAF5CL,EAAQ,CAACM,cAAyBN,UAAAA,QAAWO,IAmC7D,OA/BArB,EAAuBe,EAAI,UAAWH,EAASU,MAE/CtB,EAAuBe,EAAI,YAkCFF,EAlCgCD,EAASW,KAmC1DC,SAAAA,GACN,IAAQC,EACR,IACEA,EAAOC,KAAKC,MAAMH,EAAIC,KAIvB,CAHC,MAAOG,GAEP,MACD,CAEDf,EAAGY,EACL,IA3CAzB,EAAuBe,EAAI,eAAgB,SAACS,GAG1C,IAAIC,EAFJV,EAAGc,QAGH,IACEJ,EAAOC,KAAKC,MAAMH,EAAIC,KAIvB,CAHC,MAAOG,GAEP,YADAhB,EAASkB,MAAM,UAAU,0CAE1B,CAEDlB,EAASkB,MACP,IAAIC,MAAMN,EAAKO,SAAWP,EAAKK,OAAK,0BAA8BL,EAAKQ,YAE3E,GAEAjC,EAAuBe,EAAI,QAAS,SAACa,GACnC,IAAMM,EAA2B,oBAAX9B,QAA0BA,OAAO+B,SAASD,OAChDE,EAAGF,EAAwCA,+BAAAA,iBAAuB,GAChEG,EA2BtB,SAAqBT,GACnB,MAAsB,oBAAoB,OAARA,GAAgB,YACpDA,CAAA,CA7ByBU,CAAYV,GAAYA,KAAAA,EAAII,QAAO,IAAM,GAC9DpB,EAASkB,MACP,IAAIC,wFACgFK,EAAaC,GAGrG,GAEO,CACLE,YAAa,WAAqBC,OAAAA,QAAQC,QAAQ1B,EAAGc,QAAQ,EAEjE,UCjE8Ba,EAACC,GAC7B,OAAeA,EAJJC,IAAIC,WAAW,WAICF,EAASC,IAAIE,MAAM,GAAKH,EAASC,GAC9D,CCLgB,SAAoBG,EAClCC,EACAC,GAEA,IAAMC,EAAeF,EAAAA,CAAAA,EAAAA,GAErB,cADOE,EAASC,KACCC,EAAAA,WAACF,EAAUD,EAC9B,CCFA,SAASI,IACP,OAAOb,QAAQC,SACjB,YC2DmBa,EAAEC,EAAAC,GAChB,IAAAF,EAAAG,EAAA,IACFD,eAAA,KAEMA,EAAAC,cAOAD,EAAAE,EAAAC,EAAMC,KAAA,KAAAN,EAAAC,IANK,EAAhBA,IACEA,EAAAC,EAASC,KAETD,EAAIK,UAUEC,iBACFN,EAAAM,KAAAH,EAAAC,UAAaN,EAAEC,GAAAI,EAAAC,KAAA,KAAAN,EAAA,IAIfA,EAAAG,EAAAF,EACAD,EAAAO,EAAAL,YACEE,KAEDK,EAAAT,IA5ET,8BACA,SAAAU,YAEAA,EAAAzD,UAAYuD,cAAcG,EAAQC,GAClC,MAAa,QACCC,KAAGV,EAEjB,GAAAF,EAAY,CACV,MAAgB,EAANA,EAAMU,EAAAC,EACjB,GAAAE,EAAA,CAED,IACAT,IAAe,EAAAS,EAAOD,KAAWN,GAIjC,CAFM,MAAAQ,GACNV,IAAU,EAAAU,EACV,CACA,QACE,CACA,OAAAF,KAsBD,OAlBEA,KAAAT,EAAA,SAAAY,GAAM,IACL,IAAAd,EAAAc,EAAUT,EACX,EAAAS,EAAAb,EAEDE,QAA8BM,QAC5BC,EACAP,EAAAY,IAAeL,MAIlBP,EAAQY,EAAQ,EAAAf,GAGlB,MAAAa,GAEQV,EAAAY,EAAA,EAAAF,GAEL,EAGFE,CAAA,GAGA,IAqCM,SAAUC,EAAAA,UACVA,aAAAR,GAAgC,EAAXQ,EAAMf,CAE3B,CAnGD,IAAkBgB,EAAA,SACvBzD,GAAAA,IAAAA,EAAAA,EAAAA,UACAC,EAAOyD,EAAPzD,QACAH,EAAAA,EAAAA,MACA6D,EAAaD,EAAbC,cACAC,EAAAA,EAAAA,aAAAA,aAAe,GAAEC,EAAA,IAQjB,IAAMC,aAAqB9D,EAAS,iCAAiCC,EACzD8D,EACVH,EAAaI,OAAS,EAAI,IAAIC,gBAAgB,CAACC,YAAON,SAAAA,EAAcO,KAAK,OAAS,GAEd,OAAA3C,QAAAC,QAC/C2C,MAFRN,MAAWC,EAEQ,CAACM,YAAa,UAAWlE,QAD3CL,EAAQ,CAACM,cAAa,UAAYN,QAAWO,KACOyC,KAAA,SAA9DwB,GAMN,SAAAC,EAAAC,GAAA,IAIUjB,EACE5B,IAsBd,SAA2B8C,GACzB,IAAKA,EACH,MAAU1D,IAAAA,MAAM,qCAGlB,SACgB,EAEhB,aACE2D,GAAY,EACRC,GACFA,EAAOC,QAEX,CAEA,OAAO,IAAkBC,eAAiB,CACxCC,MAAK,SAACC,GACJJ,EAASF,EAAKO,YACd,MAAgB,kBACH,GAEbL,EACGM,OACAnC,KAGYoC,SAAAA,EACb3B,OAEA,GAAIA,EAAO4B,KACT,OAAIT,EAEHlD,QAAAC,UAGqB,KADtB2D,EAASA,EAAOC,QACLrB,QACTe,EAAWlE,QACXW,QAAAC,YAGFsD,EAAWO,QAAQ5E,KAAKC,MAAMyE,IAC9BL,EAAWlE,2BAOb,IAFA,IAAM0E,GADNH,GAAUI,EAAQC,OAAOlC,EAAOf,MAAO,CAACkD,QAAQ,KAC3BC,MAAM,MAElBC,EAAI,EAAGA,EAAIL,EAAMvB,OAAS,IAAK4B,EAAG,CACzC,MAAaL,EAAMK,GAAGP,OACtB,GAAoB,IAAhBQ,EAAK7B,OAIT,IACEe,EAAWO,QAAQ5E,KAAKC,MAAMkF,GAK/B,CAJC,MAAOjF,GAGP,OAFAmE,EAAWjE,MAAMF,GACjBgE,qBAED,CACF,CAID,GAFAQ,EAASG,EAAMA,EAAMvB,OAAS,IAEzBW,EACH,OAAAnD,QAAAC,UACD,IAAAqE,0CAGqBnB,EAAOM,QAAMnC,KAAA,SAAAiD,GAAjCb,EAAkCa,EAAA,4DAHnCC,CAAA,EAIA,SAAQpF,GACPmE,EAAWjE,MAAMF,EAClB,gEACF,CAAA,MAAAyC,GAAA,OAAA7B,QAAAyE,OAAA5C,EAAA,CAAA,GAnDqB,MACb,SAACzC,GAAQmE,OAAAA,EAAWjE,MAAMF,EAAI,EAmDzC,EAEAgE,OAAAA,GAEJ,CA1GiBsB,CAAkB5B,EAASG,MACpCE,EAASe,EAAOV,YAEhBmB,EAA8B,GAGjCC,gkBAAAC,CAAA,WAAA,OAAA7E,QAAAC,QACckD,EAAOM,QAAMnC,KAAA,SAAAwD,GAG5B,GAkGJ,SAAuB/C,GACrB,QAAKA,GAIC,aAA8C,iBAAXA,EAACzC,OAAuC,OAAjByC,EAAOzC,OAKrE,gBAAiByC,EAAOzC,OAC6B,iBAA7CyC,EAAuBzC,MAAMyF,eACnC,QAAShD,EAEf,CAhHQiD,CAFJ7E,GADA4B,EAA4B+C,GACV9D,OAGhB,MAAUzB,IAAAA,kCAAkCY,EAASb,OAGtD,GAFUa,IAA+BA,EAyHhCC,IAAIC,WAAW,OAxHvBsE,EAAUM,KAAK9E,GAGbgC,GAAiBwC,EAAUnC,OAASL,EAEtC,MADAgB,EAAOC,OAAO,8BACC7D,MAAA,6CACgC4C,EAC9C,0GAAA,EAEJ,EAAQ,WAAA,OAACJ,EAAO4B,IAAI,GAEdgB,OAAAA,GAAAA,EAAAA,KAAAA,EAAAA,KAAAA,SAAAA,GAAAA,OAAAA,CAAS,GAATA,CAAS,CAAA,IAAAO,EAAA,WAAA,GA5BQ,MAApBpC,EAASqC,8BACgDrC,EAASsC,yBAApE,MAAM,IAAS7F,MAAA,6BAgIG,iBADJ0D,EA/H6DoC,IAgI7C,aAAmB,YAAapC,IAClDzD,SAAWyD,EAAK3D,MAGvB,oBALT,IAAkB2D,CA/HgE,GA2BhE,mCACjB,CAAA,MAAApB,GAAA,OAAA7B,QAAAyE,OAAA5C,EAAA,CAAA,2GCnDeyD,SAAUnH,GAAc,IAAAoH,cCJtC,IACiBC,EADA,CAAC,cAAe,iBAAkB,SACtBC,OAAO,SAACC,GAAG,QAAOA,KAAa9H,OAAC,GAE7D,GAAI4H,EAAYhD,OAAS,EACvB,MAAM,IAASjD,MAAA,gDAAiDiG,EAAY7C,KAAK,MAErF,CDDEgD,GAEA,IAAiB7H,SAAAyH,EAAGpH,EAAOL,eAAeF,OAAOE,YAEjD,GAAIK,EAAOG,MAAO,CAChB,IAAKH,EAAOL,YACV,UAAeyB,MACb,mFAGJ,GAAIpB,EAAOL,cAAgBF,OAAOE,YAChC,MAAUyB,IAAAA,MACR,sKAGL,CAED,OEfc+F,SAAUnH,EAAgByH,GAkCzB3D,IA7BoCxD,IAiBpCoH,SAAeC,EAAmBvD,GACzCwD,OAAAA,QAAAA,QAAAA,KAAazE,KAAA,WACnB,IAAM0E,EAAO7G,EAAKA,MAAC2G,EAAW,CAACvD,OAAAA,IAAQ,OAClB0D,QAAAA,QAAAA,EAAAA,SAASD,EAAa,CAACvH,QAASkG,EAAWpC,OAAAA,KAAQjB,KAAA,SAAlES,GACN,OAAaA,EAACmE,KAAK,IACrB,EApB0BH,EAAA,WAAA,IAUvB,OATItH,IACHA,EJLU0H,SACdhI,EACAiI,EAAgDlE,GAoDjCmE,MAnDdpE,EAAAA,aAAcnE,EAAWoE,EAAXpE,YAGJU,EAOPL,EAPFK,UACAC,EAMEN,EANFM,QAEA6H,EAIEnI,EAJFmI,cACAnE,EAGEhE,EAHFgE,cACA7D,EAEEH,EAFFG,MACA8D,EACEjE,EADFiE,aAGF,IAFIjE,EALFF,OAWA,MAAO,CAAC8B,YAAac,EAAM0F,OAHZtE,EAAa,CAACzD,UAAAA,EAAWC,QAAAA,EAAS0D,cAAAA,EAAe7D,MAAAA,EAAO8D,aAAAA,IACpEd,KAAKkF,GACLlF,KAAKT,IAIV,IAG2C8D,EAMd8B,EACzBC,EAUwCC,EACxCC,EACAC,EAtBEC,EAAmB,IAAiCC,IAM9CnD,EAAoB,GAK1B2C,EAAS,IAAWvG,QAAO,SAACC,EAASwE,GACzCgC,EAAgBxG,EAChByG,EAAcjC,CAChB,GAgBA,MAAO,CAAC1E,YANS9B,EAAOH,EAAaK,EAAQ,CAC3CY,KAeF,SAA4BC,GACtB2F,GA+BN,SAAuB3F,GACrB,GAAKA,EAAIgI,UAAWhI,EAAIiI,WAAW5G,WAAW,MAA9C,CAIA,IAAcF,EAAG2G,EAAiBZ,IAAIlH,EAAIiI,aAAe,MAI3D,SAAyBC,EAAY/G,GACnC,IAAagH,EAAGL,EAAiBZ,IAAIgB,GAC3BE,EAAGzC,GAAa,GACZ0C,EAAGF,EAAUC,EAAKE,QAAQH,IAAY,GAElC,IAAdE,GAAmBlH,GAErBiH,EAAKnC,KAAK9E,GACV2G,EAAiBS,IAAIL,EAAI/G,IAChBA,GAETiH,EAAKI,OAAOH,EAAU,EAAGlH,GACzB2G,EAAiBS,IAAIL,EAAI/G,KAGzBiH,EAAKI,OAAOH,EAAU,GACtBP,EAAuB,OAACI,GAE5B,CArBEO,CAAgBzI,EAAIiI,WAAY1G,EAAqBJ,EAAUnB,EAAIgI,QAAQU,OAH1E,CAIH,CArCIC,CAAc3I,GAOlB,SAAwBoI,EAAwBpI,GAC9C4I,aAAaf,GAETD,IAAgB5H,EAAI6I,eAAiBlB,GAGvCH,EAASG,GACTC,OAAc/H,IAEd+H,EAAc5H,EAAI6I,cAClBlB,EAAaS,EAAK9G,SAGpBuG,EAAeiB,WAAWtB,EA1FV,GA0FiCY,EAAK9G,QACxD,CApBIyH,CAAepD,EAAW3F,IAE1B4E,EAAOqB,KAAKjG,EAEhB,EArBEF,gBAMmB,IACGmD,OAAAA,QAAAA,QAAAA,EAAa,CAACzD,UAAAA,EAAWC,QAAAA,EAAS0D,cAAAA,EAAe7D,MAAAA,EAAO8D,aAAAA,KAAxE4F,KAAAA,SAAAA,IACNrD,EAoEJ,SACEA,EACAsD,GAGA,MAAe,IAAkClB,IAoCjD,OAnCAkB,EAAUC,QAAQ,SAACC,GACjB,MAAcC,EAAOlC,IAAIiC,EAASlB,aAAe,GACjDoB,EAAMpD,KAAKkD,GACXC,EAAOb,IAAIY,EAASlB,WAAYoB,EAClC,GAGAD,EAAOF,QAAQ,SAACG,EAAOnB,GACrB,IAAM/G,EAAWwE,EAAU2D,KAAK,SAAC9H,GAAG,OAAQA,EAACJ,MAAQ8G,CAAE,GACvD,GAAK/G,EAAL,CASA,IAAoBoI,GAAG,EACnBpB,EAAiChH,EACrCkI,EAAMH,QAAQ,SAACC,IACbI,EAAmBA,GAAoBJ,EAASK,cAAgBrI,EAASQ,OAKrEwH,EAASnB,UACXG,EAAU5G,EAAqB4G,EAASgB,EAASnB,QAAQU,OAE7D,GAGA/C,EAAU6C,OAAO7C,EAAU2C,QAAQnH,GAAW,EAAGgH,EAlBhD,MAFCsB,QAAQC,KAAK,4CAA6CxB,EAqB9D,GAEOvC,CACT,CA9GgBgE,CAAuBX,EAASpE,IAClCsE,QAAQ,SAAC1H,GAAQsG,OAAAA,EAAiBS,IAAI/G,EAAIJ,IAAKI,EAAI,GAC7DgG,EAAS7B,GACT8B,GAAe,EAChB,CAAA,MAAA5E,GAAA,OAAA7B,QAAAyE,OAAA5C,EAAA,CAAA,EAXCvC,MAAO,SAACA,GAAY,SAAiBA,EAAM,IAGfS,YAAawG,OAAAA,GAmC3C,SAASC,EAASY,GAChBT,OAAa9H,EACbgI,OAAehI,EACf+H,OAAc/H,EACduH,EAAeE,EA6EnB,SAAiB3B,GACf,IAAeiE,EAAG,IAAiC7B,IAanD,OAXApC,EAAUuD,QAAQ,SAAC1H,GACjB,IAAcqI,EAAGD,EAAU1C,IAAIhG,EAAeM,IAC1CA,EAAIJ,IAAIC,WAAW,WAErBuI,EAAUrB,IAAIrH,EAAeM,GAYnC,SAAiCA,GAC/B,OAAWA,EAAAA,CAAAA,EAAAA,EAAKJ,CAAAA,IAAKF,EAAeM,IACtC,CAdyCsI,CAAwBtI,IACjDqI,GAEVD,EAAUrB,IAAI/G,EAAIJ,IAAKI,EAE3B,GAEYuI,MAACC,KAAKJ,EAAUK,SAC9B,CA5FmCC,CAAQ9B,GAAQA,EACjD,CA8BF,CIrHgBjB,CACRhI,EACA,SAACiJ,GACCzC,EAAYyC,EACZ+B,GACF,EACAvD,IAEH5F,QAAAC,QAEKxB,EAAQ8H,QAChBjF,KAAA,WAAA,EAnBA,CAmBC,MAnBDO,GAAA,OAAA7B,QAAAyE,OAAA5C,EAAA,CAAA,IAAkC,GACZsH,EAAGC,EAAQA,SAACjL,EAAOkL,wBAA0B,GAmFnE,WACEC,EAAoBpB,QAAQqB,EAC9B,GApFyBD,EAAuB,GAmEhD,WAAkCE,GAChC,OAAO3D,EAAM2D,EAAa3D,MAAO2D,EAAajH,QAC3CjB,KAAK,SAACmI,GACD,mBAAgCD,GAAIE,EAAS,QAACF,EAAaG,eAAgBF,KAI/ED,EAAaG,eAAiBF,EAC9BD,EAAa5H,cAAS/C,EAAW4K,GACnC,SACO,SAACrK,GACNoK,EAAa5H,SAASxC,EACxB,EACJ,CAWA,MAAO,CAACyG,MAAAA,EAAO+D,YAjEAA,SAAY3C,GACnBlB,OAAAA,QAAAA,QAAAA,KAAazE,KAAA,WACnB,OAAYuE,EAACgE,EAAAA,QAAIC,MAAA,CAAA,8CAAAA,MAAoB,CAAC5C,GAAID,WAAY,EACxD,EA8D4BhF,aA5DbA,SAAa8H,GACpBhE,OAAAA,QAAAA,QAAAA,KAAazE,KAAA,WACnB,IAAM0I,EAAaD,EAAYE,IAAI,SAAC/C,sBAAoBA,EAAE,OAAA,GAASvE,KAAK,OACxE,OAAOkD,EAAUmE,IAAAA,MAAc,EACjC,EAwD0CE,UAtD1C,SACEpE,EACAvD,EACAX,GAEA,IAAKzD,EAAOF,OACV,MAAUsB,IAAAA,MAAM,mDAMlB,IAAkBiK,EAAG,CAAC3D,MAAOC,EAAWvD,OAAAA,EAAQX,SAAAA,GAChD0H,EAAoBrE,KAAKuE,GAEzB,IAAIW,GAAe,EAYnB,OADAZ,EAAyBC,GAClB,CAACzJ,YAXY,WAClB,OAAIoK,IAIJA,GAAe,EACfb,EAAoB9B,OAAO8B,EAAoBhC,QAAQkC,GAAe,IAJ7DxJ,QAAQC,SAMnB,EAIF,EA0BqDZ,MALrD,WAEE,OADA8J,EAAiB/F,SACH3E,EAAGA,EAAQsB,cAAgBC,QAAQC,SACnD,EAGF,EFhFsB9B,EAAQ,CAC1BL,YAAAA,EACAmE,aAAAA,GAEJ"}